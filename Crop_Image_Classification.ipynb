{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive"
      ],
      "metadata": {
        "id": "kiohnyeHxsrN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s8JYQ0kwLST",
        "outputId": "e59e6b79-b1f5-485d-b8a7-6c9819f8a34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mounting the drive dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import necessary libraries"
      ],
      "metadata": {
        "id": "z_V3cB6n1AYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join"
      ],
      "metadata": {
        "id": "uplDbk88zRUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Multiple images on a folder in OpenCv (python)"
      ],
      "metadata": {
        "id": "QySXmcdz1FCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mypath='/content/drive/MyDrive/Computer_Vision/examination/crops/'\n",
        "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
        "images = np.empty(len(onlyfiles), dtype=object)\n",
        "for n in range(0, len(onlyfiles)):\n",
        "  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )"
      ],
      "metadata": {
        "id": "szU__2iey86m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets list the folder in our dataset ,here the folder is crop200 files"
      ],
      "metadata": {
        "id": "fHPGpk8shWWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Computer_Vision/examination/crops/"
      ],
      "metadata": {
        "id": "SijsCJT_hKld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffb87f7-5797-4fb4-f7d7-4ac53ec5b484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "banana\t      frame280.jpg  frame359.jpg  frame438.jpg\tframe546.jpg\n",
            "banana.jpg    frame281.jpg  frame360.jpg  frame439.jpg\tframe547.jpg\n",
            "cassava       frame282.jpg  frame361.jpg  frame440.jpg\tframe548.jpg\n",
            "cassava.jpg   frame283.jpg  frame362.jpg  frame441.jpg\tframe549.jpg\n",
            "frame101.jpg  frame284.jpg  frame363.jpg  frame442.jpg\tframe550.jpg\n",
            "frame102.jpg  frame285.jpg  frame364.jpg  frame443.jpg\tframe551.jpg\n",
            "frame103.jpg  frame286.jpg  frame365.jpg  frame444.jpg\tframe552.jpg\n",
            "frame105.jpg  frame287.jpg  frame366.jpg  frame445.jpg\tframe553.jpg\n",
            "frame106.jpg  frame288.jpg  frame367.jpg  frame446.jpg\tframe554.jpg\n",
            "frame107.jpg  frame289.jpg  frame368.jpg  frame447.jpg\tframe555.jpg\n",
            "frame211.jpg  frame290.jpg  frame369.jpg  frame448.jpg\tframe556.jpg\n",
            "frame212.jpg  frame291.jpg  frame370.jpg  frame449.jpg\tframe557.jpg\n",
            "frame213.jpg  frame292.jpg  frame371.jpg  frame450.jpg\tframe558.jpg\n",
            "frame214.jpg  frame293.jpg  frame372.jpg  frame451.jpg\tframe559.jpg\n",
            "frame215.jpg  frame294.jpg  frame373.jpg  frame452.jpg\tframe560.jpg\n",
            "frame216.jpg  frame295.jpg  frame374.jpg  frame453.jpg\tframe561.jpg\n",
            "frame217.jpg  frame296.jpg  frame375.jpg  frame454.jpg\tframe562.jpg\n",
            "frame218.jpg  frame297.jpg  frame376.jpg  frame455.jpg\tframe563.jpg\n",
            "frame219.jpg  frame298.jpg  frame377.jpg  frame456.jpg\tframe564.jpg\n",
            "frame220.jpg  frame299.jpg  frame378.jpg  frame457.jpg\tframe565.jpg\n",
            "frame221.jpg  frame300.jpg  frame379.jpg  frame458.jpg\tframe566.jpg\n",
            "frame222.jpg  frame301.jpg  frame380.jpg  frame459.jpg\tframe567.jpg\n",
            "frame223.jpg  frame302.jpg  frame381.jpg  frame460.jpg\tframe568.jpg\n",
            "frame224.jpg  frame303.jpg  frame382.jpg  frame461.jpg\tframe569.jpg\n",
            "frame225.jpg  frame304.jpg  frame383.jpg  frame462.jpg\tframe570.jpg\n",
            "frame226.jpg  frame305.jpg  frame384.jpg  frame463.jpg\tframe571.jpg\n",
            "frame227.jpg  frame306.jpg  frame385.jpg  frame464.jpg\tframe572.jpg\n",
            "frame228.jpg  frame307.jpg  frame386.jpg  frame465.jpg\tframe573.jpg\n",
            "frame229.jpg  frame308.jpg  frame387.jpg  frame466.jpg\tframe574.jpg\n",
            "frame230.jpg  frame309.jpg  frame388.jpg  frame467.jpg\tframe575.jpg\n",
            "frame231.jpg  frame310.jpg  frame389.jpg  frame468.jpg\tframe576.jpg\n",
            "frame232.jpg  frame311.jpg  frame390.jpg  frame469.jpg\tframe577.jpg\n",
            "frame233.jpg  frame312.jpg  frame391.jpg  frame470.jpg\tframe578.jpg\n",
            "frame234.jpg  frame313.jpg  frame392.jpg  frame471.jpg\tframe579.jpg\n",
            "frame235.jpg  frame314.jpg  frame393.jpg  frame472.jpg\tframe580.jpg\n",
            "frame236.jpg  frame315.jpg  frame394.jpg  frame473.jpg\tframe581.jpg\n",
            "frame237.jpg  frame316.jpg  frame395.jpg  frame503.jpg\tframe582.jpg\n",
            "frame238.jpg  frame317.jpg  frame396.jpg  frame504.jpg\tframe583.jpg\n",
            "frame239.jpg  frame318.jpg  frame397.jpg  frame505.jpg\tframe584.jpg\n",
            "frame240.jpg  frame319.jpg  frame398.jpg  frame506.jpg\tframe585.jpg\n",
            "frame241.jpg  frame320.jpg  frame399.jpg  frame507.jpg\tframe586.jpg\n",
            "frame242.jpg  frame321.jpg  frame400.jpg  frame508.jpg\tframe587.jpg\n",
            "frame243.jpg  frame322.jpg  frame401.jpg  frame509.jpg\tframe588.jpg\n",
            "frame244.jpg  frame323.jpg  frame402.jpg  frame510.jpg\tframe589.jpg\n",
            "frame245.jpg  frame324.jpg  frame403.jpg  frame511.jpg\tframe590.jpg\n",
            "frame246.jpg  frame325.jpg  frame404.jpg  frame512.jpg\tframe591.jpg\n",
            "frame247.jpg  frame326.jpg  frame405.jpg  frame513.jpg\tframe592.jpg\n",
            "frame248.jpg  frame327.jpg  frame406.jpg  frame514.jpg\tframe593.jpg\n",
            "frame249.jpg  frame328.jpg  frame407.jpg  frame515.jpg\tframe594.jpg\n",
            "frame250.jpg  frame329.jpg  frame408.jpg  frame516.jpg\tframe595.jpg\n",
            "frame251.jpg  frame330.jpg  frame409.jpg  frame517.jpg\tframe596.jpg\n",
            "frame252.jpg  frame331.jpg  frame410.jpg  frame518.jpg\tframe597.jpg\n",
            "frame253.jpg  frame332.jpg  frame411.jpg  frame519.jpg\tframe598.jpg\n",
            "frame254.jpg  frame333.jpg  frame412.jpg  frame520.jpg\tframe599.jpg\n",
            "frame255.jpg  frame334.jpg  frame413.jpg  frame521.jpg\tframe600.jpg\n",
            "frame256.jpg  frame335.jpg  frame414.jpg  frame522.jpg\tframe601.jpg\n",
            "frame257.jpg  frame336.jpg  frame415.jpg  frame523.jpg\tframe602.jpg\n",
            "frame258.jpg  frame337.jpg  frame416.jpg  frame524.jpg\tframe603.jpg\n",
            "frame259.jpg  frame338.jpg  frame417.jpg  frame525.jpg\tframe604.jpg\n",
            "frame260.jpg  frame339.jpg  frame418.jpg  frame526.jpg\tframe605.jpg\n",
            "frame261.jpg  frame340.jpg  frame419.jpg  frame527.jpg\tframe606.jpg\n",
            "frame262.jpg  frame341.jpg  frame420.jpg  frame528.jpg\tframe607.jpg\n",
            "frame263.jpg  frame342.jpg  frame421.jpg  frame529.jpg\tframe608.jpg\n",
            "frame264.jpg  frame343.jpg  frame422.jpg  frame530.jpg\tframe609.jpg\n",
            "frame265.jpg  frame344.jpg  frame423.jpg  frame531.jpg\tframe610.jpg\n",
            "frame266.jpg  frame345.jpg  frame424.jpg  frame532.jpg\tframe611.jpg\n",
            "frame267.jpg  frame346.jpg  frame425.jpg  frame533.jpg\tframe612.jpg\n",
            "frame268.jpg  frame347.jpg  frame426.jpg  frame534.jpg\tframe613.jpg\n",
            "frame269.jpg  frame348.jpg  frame427.jpg  frame535.jpg\tframe614.jpg\n",
            "frame270.jpg  frame349.jpg  frame428.jpg  frame536.jpg\tframe615.jpg\n",
            "frame271.jpg  frame350.jpg  frame429.jpg  frame537.jpg\tframe616.jpg\n",
            "frame272.jpg  frame351.jpg  frame430.jpg  frame538.jpg\tframe631.jpg\n",
            "frame273.jpg  frame352.jpg  frame431.jpg  frame539.jpg\tframe815.jpg\n",
            "frame274.jpg  frame353.jpg  frame432.jpg  frame540.jpg\tframe934.jpg\n",
            "frame275.jpg  frame354.jpg  frame433.jpg  frame541.jpg\tlabelled_data.json\n",
            "frame276.jpg  frame355.jpg  frame434.jpg  frame542.jpg\n",
            "frame277.jpg  frame356.jpg  frame435.jpg  frame543.jpg\n",
            "frame278.jpg  frame357.jpg  frame436.jpg  frame544.jpg\n",
            "frame279.jpg  frame358.jpg  frame437.jpg  frame545.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement SimpleDatasetLoader"
      ],
      "metadata": {
        "id": "h8YOZj8KhefT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Class to load the dataset images from drivce\n",
        "class SimpleDatasetLoader:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, preprocessors=None):\n",
        "        \"\"\"\n",
        "        :param preprocessors: List of image preprocessors\n",
        "        \"\"\"\n",
        "        self.preprocessors = preprocessors\n",
        "\n",
        "        if self.preprocessors is None:\n",
        "            self.preprocessors = []\n",
        "\n",
        "    # Method: Used to load a list of images for pre-processing\n",
        "    def load(self, image_paths, verbose=-1):\n",
        "        \"\"\"\n",
        "        :param image_paths: List of image paths\n",
        "        :param verbose: Parameter for printing information to console\n",
        "        :return: Tuple of data and labels\n",
        "        \"\"\"\n",
        "        data, labels = [], []\n",
        "\n",
        "        for i, image_path in enumerate(image_paths):\n",
        "            image = cv2.imread(image_path)\n",
        "            label = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "            if self.preprocessors is not None:\n",
        "                for p in self.preprocessors:\n",
        "                    image = p.preprocess(image)\n",
        "\n",
        "            data.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "            if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
        "                print('[INFO]: Processed {}/{}'.format(i+1, len(image_paths)))\n",
        "\n",
        "        return (np.array(data), np.array(labels))"
      ],
      "metadata": {
        "id": "2QfWP3Sfhhix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing SimplePreprocessor"
      ],
      "metadata": {
        "id": "YfrPCTr5hp0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Preprocessror \n",
        "class SimplePreprocessor:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, width, height, interpolation=cv2.INTER_AREA):\n",
        "        \"\"\"\n",
        "        :param width: Image width\n",
        "        :param height: Image height\n",
        "        :param interpolation: Interpolation algorithm\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    # Method: Used to resize the image to a fixed size (ignoring the aspect ratio)\n",
        "    def preprocess(self, image):\n",
        "        \"\"\"\n",
        "        :param image: Image\n",
        "        :return: Re-sized image\n",
        "        \"\"\"\n",
        "        return cv2.resize(image, (self.width, self.height), interpolation=self.interpolation)"
      ],
      "metadata": {
        "id": "9t0M4Mx-g6Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read cassava image"
      ],
      "metadata": {
        "id": "Tgsw-Kek25oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using cv2.imread() method\n",
        "cassava = '/content/drive/MyDrive/Computer_Vision/examination/crop200/cassava.png'\n",
        "img_cassava = cv2.imread(cassava)"
      ],
      "metadata": {
        "id": "wfecyDig28qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the cassava "
      ],
      "metadata": {
        "id": "HsG9bsKh6e3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import sys\n",
        "img = cv2.imread(img_cassava)\n",
        "hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "cv2_imshow(hsv_image)\n",
        "\n",
        "while True:\n",
        "    cv2.imshow(\"Sheep\", img)\n",
        "    cv2.waitKey(0)\n",
        "    sys.exit() # to exit from all the processes\n",
        " \n",
        "cv2.destroyAllWindows() # destroy all windows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "chWYQs_o5-Mh",
        "outputId": "e54b3184-9918-43b2-a8b1-5d7adfa27e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3ffd3996f849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cassava\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhsv_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhsv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effects of Filter and edge detectors on cassava"
      ],
      "metadata": {
        "id": "uDt-3lRbHMN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Median filter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B6HUsFdxHuhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cassava color histogram"
      ],
      "metadata": {
        "id": "0XYNPFEd7JrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "channels = cv2.split(img_cassava)\n",
        "colors = (\"b\", \"g\", \"r\")\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Color Histogram\")\n",
        "plt.xlabel(\"Bins\")\n",
        "plt.ylabel(\"# of Pixels\")\n",
        "features = []\n",
        "\n",
        "# Loop over the image channels (B, G, R)\n",
        "for (channel, color) in zip(channels, colors):\n",
        "\n",
        "    # Calculate histogram\n",
        "    hist = cv2.calcHist([channel], [0], None, [255], [1, 256])\n",
        "    features.extend(hist)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.plot(hist, color = color)\n",
        "    plt.xlim([0, 256])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4mv-LRBv7NSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT feature extarction of cassava plant"
      ],
      "metadata": {
        "id": "tPkDo2CuJfCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install opencv-contrib-python\n",
        "\n",
        "# Important NOTE:  Use opencv <= 3.4.2.16 as\n",
        "# SIFT is no longer available in\n",
        "# opencv > 3.4.2.16\n",
        "import cv2\n",
        " \n",
        "# Loading the image\n",
        "img = img_cassava\n",
        " \n",
        " # Converting image to grayscale\n",
        "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        " \n",
        "# Applying SIFT detector\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "kp = sift.detect(gray, None)\n",
        " \n",
        "# Marking the keypoint on the image using circles\n",
        "img=cv2.drawKeypoints(gray ,\n",
        "                      kp ,\n",
        "                      img ,\n",
        "                      flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        " \n",
        "cv2.imwrite('image-with-keypoints.jpg', img)\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "EmP1KyoCJnhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Read Banana Image"
      ],
      "metadata": {
        "id": "iW9bsl2F32mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using cv2.imread() method\n",
        "banana = '/content/drive/MyDrive/Computer_Vision/Garden_Crop_Images/banana.png'\n",
        "img_banana = cv2.imread(banana)\n",
        "img_banana"
      ],
      "metadata": {
        "id": "WPlBmLQa32L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the banana"
      ],
      "metadata": {
        "id": "qkcmHUDi6n1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "hsv_image = cv2.cvtColor(img_banana, cv2.COLOR_BGR2HSV)\n",
        "cv2_imshow(hsv_image)"
      ],
      "metadata": {
        "id": "o13pIhXR6SKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Banana color histogram"
      ],
      "metadata": {
        "id": "Lj40Clt-76_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "channels = cv2.split(img_banana)\n",
        "colors = (\"b\", \"g\", \"r\")\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Color Histogram\")\n",
        "plt.xlabel(\"Bins\")\n",
        "plt.ylabel(\"# of Pixels\")\n",
        "features = []\n",
        "\n",
        "# Loop over the image channels (B, G, R)\n",
        "for (channel, color) in zip(channels, colors):\n",
        "\n",
        "    # Calculate histogram\n",
        "    hist = cv2.calcHist([channel], [0], None, [255], [1, 256])\n",
        "    features.extend(hist)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.plot(hist, color = color)\n",
        "    plt.xlim([0, 256])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqRMH44V7vnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT feature extraction algorithm for banana"
      ],
      "metadata": {
        "id": "_SCpMTJ0EAP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python\n",
        "\n",
        "# Important NOTE:  Use opencv <= 3.4.2.16 as\n",
        "# SIFT is no longer available in\n",
        "# opencv > 3.4.2.16\n",
        "import cv2\n",
        " \n",
        "# Loading the image\n",
        "img = img_banana\n",
        " \n",
        " # Converting image to grayscale\n",
        "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        " \n",
        "# Applying SIFT detector\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "kp = sift.detect(gray, None)\n",
        " \n",
        "# Marking the keypoint on the image using circles\n",
        "img=cv2.drawKeypoints(gray ,\n",
        "                      kp ,\n",
        "                      img ,\n",
        "                      flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        " \n",
        "cv2.imwrite('image-with-keypoints.jpg', img)\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "vckUCQkLEGmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k-NN classifier for image classification"
      ],
      "metadata": {
        "id": "W2SgGPogMA4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Step #1 –** Gather Our Dataset**: The datasets consists of 3,000 images with 1,000 images per dog, cat, and panda class, respectively. Each image is represented in the RGB76 color space. We will preprocess each image by resizing it to 32 × 32 pixels. Taking into account the three RGB channels, the resized image dimensions imply that each image in the dataset is represented by 32 × 32 × 3 = 3, 072 integers.\n",
        "\n",
        "• Step #2 – Split the Dataset: We will split the data, One split for training, and the other for testing. • Step #3 – Train the Classifier: Our k-NN classifier will be trained on the raw pixel intensi- ties of the images in the training set.\n",
        "\n",
        "• Step #4 – Evaluate: Once our k-NN classifier is trained, we can evaluate performance on the test set."
      ],
      "metadata": {
        "id": "m7Or_pjCh3Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/drive/MyDrive/Computer_Vision/Garden_Crop_Images\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(data, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (3000, 32, 32, 3) to (3000, 32*32*3=3072)\n",
        "data = data.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(data.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: Classification starting....')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))"
      ],
      "metadata": {
        "id": "lLE-2Uvvgp1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Best K"
      ],
      "metadata": {
        "id": "96U0zPXdillM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np \n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=2,n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "accuracy = accuracy_score(model.predict(test_x), test_y)\n",
        "print(accuracy)\n",
        "n_neighbors = np.array([7,8,9,10,12,15,20])\n",
        "param_grid = dict(n_neighbors=n_neighbors)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid.fit(train_x, train_y)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_estimator_.n_neighbors)"
      ],
      "metadata": {
        "id": "oh44oPx0iiTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#print(grid.cv_results_)\n",
        "print(grid.param_grid)\n",
        "print(grid.best_score_)\n",
        "print(grid.scorer_)"
      ],
      "metadata": {
        "id": "Gc34pcotjBG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make KNN Faster"
      ],
      "metadata": {
        "id": "9NKxp5xsjJ3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #algorithm='ball_tree'\n",
        " #its fs\n",
        "  \n",
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/drive/MyDrive/Computer_Vision/Garden_Crop_Image\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(data, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (3000, 32, 32, 3) to (3000, 32*32*3=3072)\n",
        "data = data.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(data.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: Classification starting....')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1,algorithm='kd_tree')\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))"
      ],
      "metadata": {
        "id": "xUBFAehEjMU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}